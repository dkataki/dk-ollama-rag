{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets import all necessary libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to extract text from PDF ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path) :\n",
    "    reader = PdfReader ( pdf_path)\n",
    "    text = \" \"\n",
    "    for page in reader.pages:\n",
    "        text += page .extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create FAISS vector store ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_store(text, path=\"false_index\") :\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "    chunks = splitter.split_text(text)\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    vector_store.save_local(path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load FAISS vector store ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_vector_store(path=\"faiss_index\") :\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vector_store = FAISS.load_local(path,embeddings,allow_dangerous_deserialization=True)\n",
    "    return vector_store\n",
    "\n",
    "# BUild QA Chain \n",
    "def build_qa_chain(vector_store_path=\"faiss_index\") :\n",
    "    vector_store = load_faiss_vector_store(vector_store_path)\n",
    "    retriver = vector_store.as_retriever()\n",
    "    # Load QA chain for combining our documents\n",
    "    llm = Ollama(model=\"llama3.2\")\n",
    "    qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    qa_chain = RetrievalQA(retriver=retriver,combine_document_chain=qa_chain)\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now comes the Streamlit APP part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"RAG Chatbot with FAISS and LLaMA\")\n",
    "st.write(\"Upload a PDF and ask questions based on its content.\")\n",
    "uploaded_file = st.file_uploader(\"Upload your PDF file\", type=\"pdf\")\n",
    "if uploaded_file is not None:\n",
    "    pdf_path = f\"uploaded/{uploaded_file.name}\"\n",
    "    os.makedirs(\"uploaded\", exist_ok=True)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.getbuffer())\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    st.info(\"Creating FAISS vector store...\")\n",
    "    faiss_store(text)\n",
    "    st.info(\"Initializing chatbot...\")\n",
    "    qa_chain = build_qa_chain()\n",
    "    st.success(\"Chatbot is ready!\")\n",
    "\n",
    "\n",
    "if 'qa_chain' in locals():\n",
    "    question = st.text_input(\"Ask a question about the uploaded PDF:\")\n",
    "    if question:\n",
    "        st.info(\"Querying the document...\")\n",
    "        answer = qa_chain.run(question)\n",
    "        st.success(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olama-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
