
Setting Up LLaMA with Ollama

1 . Install Ollama on your respective OS.
            Check out the platform https://ollama.com/download  to download ollama
    Once installed , Make sure its up and running
           
2. Install Llama 3.2 Model
Once you have Ollama installed, you need to download the Llama 3.2 model for your chatbot. Run the following command:

#ollama pull llama3.2

3. Run the command using terminal
This command will download the Llama 3.2 model to your system, making it ready for local use.

Run Ollama locally 
#ollama run llama3

Other useful commands:

#ollama list


### Refined steps ###

Step: 1 Open your MAC terminal and run: 
#/bin/bash -c "$(curl -fsSL https://ollama.ai/install.sh)"

Step 2: Verify the Installation
Check if Ollama is installed correctly:

#ollama version

Step 3: 
For the latest Llama 3.2 version: 
ollama run llama3

For a specific size (like 1B, 8B, etc.):
ollama run llama3:8b

Step 4: 

To confirm Llama 3.2 is installed:

#ollama list

Step 5: Run Llama 3.2 Locally

#ollama run llama3
You can now interact with Llama 3.2 directly from your Mac!

Alternatively: 

Pull a model without running it:
 #ollama pull llama3
Stop Ollama:
 # pkill ollama
Remove a model:
 #ollama rm llama3



 ##https://www.youtube.com/watch?v=daZOrbMs61I